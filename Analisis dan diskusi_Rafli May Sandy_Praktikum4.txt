Analisis:
	Dalam percobaan pada minggu ke-4 ini, saya melakukan percobaan melatih dan menguji agen RL untuk mengontrol lingkungan secara otonom. Dalam percobaan ini saya menggunakan algoritma Deep Q-Network (DQN) yang berfungsi untuk mengimplementasikan RL, selain itu saya juga menggunakan TensorFlow untuk membuat dan menjalankan RL. Dalam proses pembuatan, program dibagi menjadi 2. Program pertama digunakan untuk melatih agen RL sedangkan pada program kedua digunakan untuk menguji agen RL tersebut. Ketika kedua program dijalankan menghasilkan data latih dan data uji. Dari kedua data tersebut dapat disimpulkan bahwa agen belum mampu mengontrol CartPole dengan baik dikarenakan skor pelatihan tidak meningkat secara signifikan, sehingga skor uji menjadi rendah. Namun hal ini dapat diatasi dengan melakukan perubahan parameter seperti gamma, epsilon, dan leraning rate. Ketika nilai gamma tinggi maka agen lebih focus pada reward jangka Panjang dan dapat meningkatkan stabilitas pembelajaran, sedangkan jika nilai gamma rendah agen akan lebih focus pada reward yang sudah jadi atau instan dan dapat mempercepat pembelajaran. Pada epsilon jika diberi niali tinggi agen akan lebih sering mencoba aksi acak (eksplorasi) sehingga sangat bagus digunakan untuk awal training. Jika epsilon diberi nilai rendah agen akan lebih sering memilih aksi terbaik (eksploitasi) sehingga bagus digunakan untuk akhir training. Pada learning rate jika diberi nilai tinggi membuat agen dapat belajar dengan cepat tetapi bisa tidak stabil dan rentan terhadap overfiting, sedangkan jika diberi nilai rendah dapat membuat agen menjadi lambat dalam belajar tetapi bisa membuat lebih stabil dan dapat meningkatkan performa jangka panjang.
Diskusi:
1.	Perbedaan utama antara RL dan Supervised Learning dalam sistem kendali dapat dilihat dari cara belajar dan sumber data. Pada RL dalam melakukan pembelajaran melalui trial dan error dengan mengeksplorasi lingkungan dan menerima reward dan RL tidak memerlukan dataset berlabel karena dapat mengumpulkan data sendiri dengan berinteraksi dengan lingkungan. Sedangkan Supervised Learning dalam melakukan pembelajaran melalui data berlabel yang telah diberikan sebelumnya sehingga membutuhkan dataset berlabel besar untuk melatih model dengan akurasi tinggi.
2.	Cara mengoptimalkan keseimbangan antara eksplorasi dan eksploitasi yaitu, pada awal pelatihan gunakan lebih banyak eksplorasi dengan menggunakan ε-Greedy, Boltzmann, atau UCB untuk mengeksplorasi lingkungan. Ketika sudah berjalan seiring waktu gunakan eksploitasi bisa dengan menggunakan ε-decay atau Dueling DQN untuk menemukan strategi terbaik. Jika kondisi masalah dengan reward jarang  gunakan curiosity-driven exploration.
3.	Aplikasi lain dari RL dalam sistem kendali nyata yang dapat diimplementasikan yaitu:
•	Kendali kendaraan otonom (Self-Driving Cars & Autonomous Rail).
•	Kendali Robotik dan otomasi industry.
•	Kendali pergerakan kereta api.
